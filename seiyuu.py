import os
import re
import json
import time
import logging
import requests
import schedule
from typing import Optional
from openai import OpenAI
import xml.etree.ElementTree as ET
from datetime import datetime
from html import unescape
from urllib.parse import urlparse, parse_qs
from datetime import datetime, timezone, timedelta
from PIL import Image, ImageDraw, ImageFont
from pathlib import Path
#import emoji
#from io import BytesIO
from html2image import Html2Image
from jinja2 import Template
from jinja2 import Environment, FileSystemLoader
import subprocess
import base64
import win32gui
import win32con


# -------------------
# é…ç½®åŒº
# -------------------


# åŠ è½½é…ç½®æ–‡ä»¶
CONFIG_PATH = './config.json'
with open(CONFIG_PATH, 'r', encoding='utf-8') as config_file:
    config = json.load(config_file)

# ä»é…ç½®æ–‡ä»¶ä¸­è·å–å‚æ•°
MIRAI_API_URL = config.get("MIRAI_API_URL")
VERIFY_KEY = config.get("VERIFY_KEY")
QQ_ID = config.get("QQ_ID")
TARGET_IDs = config.get("TARGET_IDs")
TARGET_IDs_list = config.get("TARGET_IDs_list")
USERNAME_LIST = config.get("USERNAME_LIST")
RSS_URLS = config.get("RSS_URLS")
RSSHUB_BAT_PATH = config.get("RSSHUB_BAT_PATH")

# Deepseek API é…ç½®
DEEPSEEK_API_KEY = config.get("api_key")
DEEPSEEK_BASE_URL = config.get("base_url")



#RSS_BASE_URL = "http://rsshub.app/twitter/user/"
#RSS_URLS = [f"{RSS_BASE_URL}{username}" for username in USERNAME_LIST]

SEEN_FILE = 'seen.json'


USER_FOLDER_MAP = {username: username for username in USERNAME_LIST}

proxies = {
    #'http': 'http://127.0.0.1:7897',
    #'https': 'http://127.0.0.1:7897',
}
AVATAR_DIR = './avatar'  # <--- æ–°å¢å¤´åƒç›®å½•

rt_pattern = re.compile(r'^(.+?)\s+RT\s*<br>', re.IGNORECASE)


# -------------------
# å·¥å…·å‡½æ•°
# -------------------


# åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆå»ºè®®æ”¾åœ¨å…¨å±€é…ç½®åŒºï¼‰

def translate_text(text: str) -> str:
    """
    ä½¿ç”¨ Deepseek API è¿›è¡Œæ–‡æœ¬ç¿»è¯‘ï¼ˆæ—¥è¯­ -> ç®€ä½“ä¸­æ–‡ï¼‰
    è¿”å›æ ¼å¼: ç¿»è¯‘åçš„æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œå¤±è´¥æ—¶è¿”å›åŸæ–‡æœ¬
    """
    # ç‹¬ç«‹å®¢æˆ·ç«¯å®ä¾‹ï¼ˆé…ç½®ç›´è¿ï¼‰
    DEEPSEEK_CLIENT = OpenAI(
        #api_key=,  # å»ºè®®æ”¹ä¸ºä»ç¯å¢ƒå˜é‡è·å–
        #base_url="https://api.deepseek.com",
        timeout=30,
        # æ­£ç¡®é…ç½®æ— ä»£ç†çš„æ–¹å¼
    )

    if not DEEPSEEK_CLIENT.api_key:
        logging.warning("Deepseek API key æœªé…ç½®ï¼Œè·³è¿‡ç¿»è¯‘")
        return text

    system_prompt = (
        "ä½ æ˜¯ä¸€åä¸“ä¸šç¿»è¯‘ï¼Œè¯·å°†æ—¥è¯­å†…å®¹ç²¾å‡†ç¿»è¯‘ä¸ºç®€ä½“ä¸­æ–‡ã€‚è¦æ±‚ï¼š\n"
        "1. ä¿æŒåŸæœ‰æ¢è¡Œå’Œæ ¼å¼\n"
        "2. ä¿ç•™#è¯é¢˜æ ‡ç­¾å’Œ@æåŠ,ä¸è¿›è¡Œç¿»è¯‘ï¼ŒåŒæ—¶#è¯é¢˜æ ‡ç­¾åå¿…é¡»ä¿ç•™ç©ºæ ¼\n"
        "3. ç¦æ­¢æ·»åŠ è§£é‡Šå†…å®¹\n"
        "4. ä¿ç•™URLé“¾æ¥ä¸å˜\n"
        "5. å¤„ç†æ—¥å¼é¢œæ–‡å­—ä¸ç¿»è¯‘\n"
        "6. äººåä¿ç•™åŸæ–‡ä¸ç¿»è¯‘"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"è¯·ç¿»è¯‘ä»¥ä¸‹å†…å®¹ï¼š\n{text}"}
    ]

    max_retries = 3
    for attempt in range(max_retries):
        try:
            start_time = time.time()
            
            response = DEEPSEEK_CLIENT.chat.completions.create(
                model="deepseek-chat",
                messages=messages,
                temperature=0.2,
                top_p=0.3,
                max_tokens=8000,
                stream=False
            )

            if response.choices and response.choices[0].message.content:
                translated = response.choices[0].message.content.strip()
                translated = re.sub(r'^ç¿»è¯‘[ï¼š:]?\s*', '', translated)
                translated = re.sub(r'\n{3,}', '\n\n', translated)
                
                logging.info(
                    f"ç¿»è¯‘æˆåŠŸ | è€—æ—¶ {time.time()-start_time:.2f}s | "
                    f"Tokenä½¿ç”¨: {response.usage.total_tokens}"
                )
                return translated

        except Exception as e:
            wait_time = 2 ** attempt
            logging.warning(f"ç¿»è¯‘å°è¯• {attempt+1}/{max_retries} å¤±è´¥: {str(e)}")
            time.sleep(wait_time)
    
    logging.error(f"æ‰€æœ‰é‡è¯•å¤±è´¥ | åŸæ–‡: {text[:100]}...")
    return text

def merge_consecutive_br(text: str) -> str:
    """åˆå¹¶æ–‡æœ¬ä¸­è¿ç»­çš„<br>æ ‡ç­¾ä¸ºå•ä¸ª<br>"""
    if not text:
        return text
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†è¿ç»­çš„<br>æ›¿æ¢ä¸ºå•ä¸ª<br>
    return re.sub(r'(?:<br>){2,}', '<br>', text)


def clean_html(text: str, author: str = None) -> str:
    if author:
        # ä½¿ç”¨RSSä¸­çš„ä½œè€…åç§°ï¼ŒæŸ¥æ‰¾å¹¶ç§»é™¤descriptionä¸­è¯¥ä½œè€…åä¹‹å‰çš„æ‰€æœ‰å†…å®¹
        pattern = re.compile(fr'.*?{re.escape(author)}[:ï¼š]\s*(.*)', re.DOTALL)  
        match = pattern.match(text)
        if match:
            text = f"{author}: {match.group(1)}"
        else:
            pattern = re.compile(r'.*?([^>]+?)[:ï¼š]\s*(.*)', re.DOTALL)
            match = pattern.match(text)
            if match:
                text = f"{author}: {match.group(2)}"
        text = re.sub(fr'^{re.escape(author)}[:ï¼š]\s*', '', text)

    text = unescape(text)
    # æ¸…ç†å…¶ä»–HTMLæ ‡ç­¾ï¼Œä¿ç•™<br>
    text = re.sub(r'<(?!br).*?>', '', text)
    # åˆå¹¶è¿ç»­çš„<br>æ ‡ç­¾
    text = merge_consecutive_br(text)
    return text

# åœ¨å·¥å…·å‡½æ•°åŒºæ·»åŠ 
def extract_user_id(link: str) -> str:
    """ä»æ¨ç‰¹URLæå–ç”¨æˆ·ID"""
    parsed = urlparse(link)
    path_segments = [s for s in parsed.path.split('/') if s]
    return path_segments[0] if path_segments else 'unknown_user'


# -------------------
# ä¸‹è½½å’Œä¸Šä¼ éƒ¨åˆ†
# -------------------

UPLOAD_RECORD_FILE = 'uploaded_files.json'
DOWNLOAD_DIR = '.\å¥³å£°ä¼˜å›¾åº“'

def load_seen():
    return set(json.load(open('seen.json', 'r')) if os.path.exists('seen.json') else [])

def save_seen(seen):
    with open('seen.json', 'w') as f:
        json.dump(list(seen), f, indent=2)

def load_uploaded():
    return set(json.load(open(UPLOAD_RECORD_FILE, 'r')) if os.path.exists(UPLOAD_RECORD_FILE) else [])

def save_uploaded(uploaded):
    with open(UPLOAD_RECORD_FILE, 'w') as f:
        json.dump(list(uploaded), f, indent=2)

# ç‰¹æ®Šè¿”å›å€¼ï¼Œç”¨äºæ ‡è¯†è·³è¿‡äº†æ¨ç‰¹å¤´åƒ
SKIPPED_PROFILE_IMAGE_FLAG = "SKIPPED_PROFILE_IMAGE"

# æ–°å¢å‡½æ•°ï¼šæ ¹æ®ä½œè€…åè·å–æœ¬åœ°å¤´åƒ
def get_avatar_by_author(author: str) -> str:
    """é€šè¿‡ä½œè€…åæŸ¥æ‰¾æœ¬åœ°å·²å­˜åœ¨çš„å¤´åƒæ–‡ä»¶"""
    safe_author = re.sub(r'[\\/:*?"<>|]', '_', author)
    for ext in ['.jpg', '.jpeg', '.png', '.gif']:
        filename = f"{safe_author}{ext}"
        path = os.path.join(AVATAR_DIR, filename)
        if os.path.exists(path):
            logging.info(f"æ‰¾åˆ°æœ¬åœ°å¤´åƒï¼š{filename}")
            return path
    logging.warning(f"æœªæ‰¾åˆ°æœ¬åœ°å¤´åƒï¼š{author}")
    return None


def download_media(url: str, author: str) -> str:
    """
    ä¸‹è½½åª’ä½“æ–‡ä»¶åˆ°æŒ‡å®šç›®å½•ï¼Œä½¿ç”¨æ—¶é—´æˆ³+åŸå§‹æ–‡ä»¶åï¼Œè‡ªåŠ¨æ£€æŸ¥é‡å¤ã€‚
    å¦‚æœ URL æ˜¯æ¨ç‰¹å¤´åƒï¼Œåˆ™è·³è¿‡ä¸‹è½½å¹¶è¿”å› SKIPPED_PROFILE_IMAGE_FLAGã€‚
    """
    # æ–°å¢ï¼šå¼ºåˆ¶è¿‡æ»¤éæ³•å­—ç¬¦
    author = re.sub(r'[\\/:*?"<>|]', '_', author)  # ç¡®ä¿ç›®å½•ååˆæ³•
    user_dir = os.path.join(DOWNLOAD_DIR, author)
    # æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºæ¨ç‰¹å¤´åƒ URL
    if url.startswith("https://pbs.twimg.com/profile_images/"):
        logging.info(f"æ£€æµ‹åˆ°æ¨ç‰¹å¤´åƒé“¾æ¥ï¼Œè·³è¿‡ä¸‹è½½å’Œåç»­ä¸Šä¼ ï¼š{url}")
        return SKIPPED_PROFILE_IMAGE_FLAG
    user_dir = os.path.join(DOWNLOAD_DIR, author)
    os.makedirs(user_dir, exist_ok=True)

    # ä¿®å¤URLç¼–ç é—®é¢˜
    url = unescape(url.replace('&amp;', '&'))
    parsed_url = urlparse(url)
    
    # æå–åŸå§‹æ–‡ä»¶åï¼ˆå¸¦æ‰©å±•åï¼‰
    original_filename = os.path.basename(parsed_url.path)
    
    # ç‰¹æ®Šå¤„ç†æ¨ç‰¹å›¾ç‰‡æ ¼å¼
    if "pbs.twimg.com/media" in url:
        query_params = parse_qs(parsed_url.query)
        if 'format' in query_params and '.' not in original_filename:
            original_filename += f".{query_params['format'][0]}"
    
    # ç”Ÿæˆæ—¶é—´æˆ³å‰ç¼€
    timestamp = datetime.now().strftime("%Y%m%d")
    
    # æ„å»ºå®Œæ•´æ–‡ä»¶å
    filename = f"{timestamp}_{original_filename}"
    path = os.path.join(user_dir, filename)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŸºäºå®Œæ•´æ–‡ä»¶åï¼‰
    if os.path.exists(path):
        logging.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ï¼š{filename}")
        return path

    # ä¸‹è½½é‡è¯•é€»è¾‘
    max_retries = 3
    for attempt in range(max_retries):
        try:
            resp = requests.get(
                url,
                proxies=proxies,
                stream=True,
                timeout=15,
                verify=False,
                headers={'User-Agent': 'Mozilla/5.0'}
            )
            resp.raise_for_status()

            # å†™å…¥æ–‡ä»¶
            with open(path, 'wb') as f:
                for chunk in resp.iter_content(1024):
                    f.write(chunk)
            
            logging.info(f"ä¸‹è½½æˆåŠŸï¼š{filename}")
            return path

        except Exception as e:
            logging.warning(f"å°è¯• {attempt+1}/{max_retries} å¤±è´¥ï¼š{str(e)[:100]}")
            time.sleep(1)
    
    logging.error(f"ä¸‹è½½å¤±è´¥ï¼š{url}")
    return None


def upload_image(file_path, session_key):
    with open(file_path, 'rb') as img_file:
        files = {'img': img_file}
        data = {'sessionKey': session_key, 'type': 'group'}
        res = requests.post(f"{MIRAI_API_URL}/uploadImage", data=data, files=files)
        return res.json().get("imageId") if res.status_code == 200 else None

def upload_video(file_path, session_key, target_id):
    with open(file_path, 'rb') as video_file:
        files = {'file': video_file}
        data = {
            'sessionKey': session_key,
            'type': 'group',
            'target': str(target_id),
            'path': ''
        }
        res = requests.post(f"{MIRAI_API_URL}/file/upload", data=data, files=files)
        return res.json().get("data", {}).get("id") if res.status_code == 200 else None

def send_message(session_key, target, message_chain):
    return requests.post(f"{MIRAI_API_URL}/sendGroupMessage", json={
        "sessionKey": session_key,
        "target": target,
        "messageChain": message_chain
    })

# -------------------
# ç”¨äºé‡å¯rsshubçš„å‡½æ•°ï¼Œè‹¥åœ¨dockeréƒ¨ç½²ï¼Œæ­¤éƒ¨åˆ†éœ€è¦é‡æ„
# -------------------
import socket

def is_rsshub_running(host='localhost', port=14607):
    try:
        with socket.create_connection((host, port), timeout=3):
            return True
    except Exception:
        return False
    

WINDOW_TITLE = "RSSHUB_CMD_WINDOW"

def close_rsshub_window():
    def callback(hwnd, extra):
        if win32gui.IsWindowVisible(hwnd):
            title = win32gui.GetWindowText(hwnd)
            if WINDOW_TITLE in title:
                print(f"æ‰¾åˆ°çª—å£ï¼š{title}ï¼Œæ­£åœ¨å…³é—­")
                win32gui.PostMessage(hwnd, win32con.WM_CLOSE, 0, 0)

    win32gui.EnumWindows(callback, None)

def restart_rsshub():
    close_rsshub_window()
    time.sleep(2)
    # ç”¨æ–°çš„ cmd çª—å£è¿è¡Œ rsshub
    subprocess.Popen(
        ["cmd.exe", "/c", "start", "", RSSHUB_BAT_PATH],
        creationflags=subprocess.CREATE_NEW_CONSOLE
    )
    print("å·²åœ¨æ–°çª—å£ä¸­é‡å¯ RSSHub")

# -------------------
# htmlè½¬å›¾ç‰‡çš„ä¸»å‡½æ•°
# -------------------

def Twitter_seiyuu():
    seen = load_seen()
    uploaded = load_uploaded()

    session_res = requests.post(f"{MIRAI_API_URL}/verify", json={"verifyKey": VERIFY_KEY})
    session_key = session_res.json().get("session")
    if not session_key:
        print("è®¤è¯å¤±è´¥")
        return
    requests.post(f"{MIRAI_API_URL}/bind", json={"sessionKey": session_key, "qq": QQ_ID})

    if not is_rsshub_running():
        logging.warning("RSSHub æœªè¿è¡Œï¼Œæ­£åœ¨å¯åŠ¨...")
        restart_rsshub()
        # å¯é€‰ï¼šç­‰å¾…å¯åŠ¨å®Œæˆ
        time.sleep(10)


    for url in RSS_URLS:
        try:
            resp = requests.get(url, timeout=40)
            resp.raise_for_status()
            root = ET.fromstring(resp.content)
            logging.info(f"æŠ“å–æˆåŠŸï¼š{url}")
        except Exception as e:
            error_msg = str(e)
            logging.error(f"æŠ“å–å¤±è´¥ï¼š{url} -> {error_msg}")

            if ("503 Server Error" in error_msg) or \
               ("HTTPConnectionPool(host='localhost', port=14607): Read timed out." in error_msg):
                logging.warning("æ£€æµ‹åˆ° RSSHub å¼‚å¸¸ï¼Œå°è¯•é‡å¯æœåŠ¡...")
                restart_rsshub()

            continue

        for item in root.findall('.//item'):
            # åœ¨æ¯ä¸ªitemå¼€å§‹æ—¶é‡ç½®å¼•ç”¨ç›¸å…³å˜é‡
            quoted_username = ''
            quote_block = ''
            quote_clean = ''
            quote_zh = ''
            avatar_quote = None

            link = item.findtext('link')
            if not link or link in seen:
                continue

            # æ–°å¢ç”¨æˆ·IDæå–
            author_id = extract_user_id(link)
            title = item.findtext('title') or ''
            desc  = item.findtext('description') or ''
            # ä¿®æ”¹å
            author_text = item.findtext('author') or 'unknown'  # ç›´æ¥è·å–authoræ ‡ç­¾å†…å®¹
            author_clean = re.sub(r'<[^>]+>', '', author_text)
            # æ¸…ç†HTMLæ ‡ç­¾å’Œç‰¹æ®Šæ ‡è®°
            author = re.sub(r'<[^>]+>', '', author_text)
            #author = re.sub(r'\s*(?:OFFICIAL|ã®ã“ã¨ã€‚?|[:ï¼š])\s*$', '', author)  # ç§»é™¤ç»“å°¾çš„OFFICIALç­‰æ ‡è®°å’Œå†’å·
            author = re.sub(r'[\\/:*?"<>|\s]', '_', author.strip()).strip('_')

            pub_dt = item.findtext('pubDate') or ''
            categories = ['#' + c.text.strip() for c in item.findall('category') if c.text]
        
            
            # --- Step 1: æ£€æµ‹ç›´æ¥RTæ¨¡å¼ ---
            # æ£€æµ‹æ˜¯å¦ä¸ºç›´æ¥è½¬å‘æ¨¡å¼
            rt_match = rt_pattern.search(desc)

            if rt_match:  # ç›´æ¥è½¬å‘æ— è¯„è®ºçš„æƒ…å†µ
                # ä¸»æ¨å†…å®¹ç½®ç©º
                desc_main_only = ''
                # æ·»åŠ æ¸…ç†é€»è¾‘
                author = re.sub(r'<[^>]+>', '', author)
                author = re.sub(r'[\\/:*?"<>|\s]', '_', author.strip()).strip('_')

                # ä¿®æ”¹ï¼šç›´æ¥ä»RTåçš„å†…å®¹æå–è½¬æ¨å—çš„å†…å®¹
                rt_content = desc[rt_match.end():]
                quote_block = rt_content

                    
                # æå–å¼•ç”¨å†…å®¹ä¸­çš„å¤´åƒå’Œç”¨æˆ·å (å’Œæ™®é€šæ¨æ–‡ä½¿ç”¨ç›¸åŒçš„æå–é€»è¾‘)
                quote_author_match = re.search(r'<img[^>]+?src="([^"]+?)"[^>]*?>([^:<]+?):', quote_block)
                if quote_author_match:
                    quote_avatar_url = unescape(quote_author_match.group(1))
                    quoted_username = quote_author_match.group(2).strip()
                    # æ¸…ç†ç”¨æˆ·åï¼Œç§»é™¤æœ«å°¾çš„æ ‡è®°å’Œå†’å·
                    quoted_username = re.sub(r'<[^>]+>', '', quoted_username)
                    #quoted_username = re.sub(r'\s*(?:OFFICIAL|ã®ã“ã¨ã€‚?|[:ï¼š])\s*$', '', quoted_username)
                    quoted_username = re.sub(r'[\\/:*?"<>|\s]', '_', quoted_username.strip()).strip('_')
                    
                    logging.info(f"æå–åˆ°RTç”¨æˆ·ä¿¡æ¯ - ç”¨æˆ·å: {quoted_username}")
                    logging.info(f"æå–åˆ°RTç”¨æˆ·ä¿¡æ¯ - å¤´åƒURL: {quote_avatar_url}")
                    
                    # æ¸…ç†å¼€å¤´çš„ä½œè€…åå’Œå†’å·
                    quote_block = re.sub(fr'{re.escape(quoted_username)}[:ï¼š]\s*', '', quote_block)  # ç§»é™¤ç”¨æˆ·åå’Œå†’å·
                    quote_block = re.sub(r'^(?:<img[^>]+>)', '', quote_block)  # åªç§»é™¤å¤´åƒæ ‡ç­¾
                
                quote_clean = clean_html(quote_block)  # ç›´æ¥æ¸…ç†RTåçš„å†…å®¹
                quote_zh = translate_text(quote_clean) if quote_clean else ''  # ç¿»è¯‘RTå†…å®¹
                logging.info(f"[{author}] æ£€æµ‹åˆ°ç›´æ¥è½¬å‘æ¨¡å¼ï¼ŒRTå†…å®¹å·²æ¸…ç†å¹¶ç¿»è¯‘ã€‚")
                
                # ä¸‹è½½å¼•ç”¨ç”¨æˆ·å¤´åƒ
                if quote_avatar_url and quoted_username and quote_avatar_url.startswith("https://pbs.twimg.com/profile_images/"):
                    avatar_quote = get_avatar_by_author(quoted_username)
                    if not avatar_quote:
                        avatar_quote = download_avatar(quote_avatar_url, quoted_username)
                    avatar_quote = get_avatar_by_author(quoted_username)

                # ç›´æ¥è½¬å‘æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ author ä¸‹è½½ç”¨æˆ·å¤´åƒ
                avatar_path = get_avatar_by_author(author)
                if not avatar_path:  # å¦‚æœæœ¬åœ°æ²¡æœ‰å¤´åƒ
                    # å°è¯•ä»descä¸­æå–åŸä½œè€…å¤´åƒURL
                    author_avatar_match = re.search(r'<img[^>]+?src="([^"]+?/profile_images/[^"]+?)"[^>]*?>', desc)
                    if author_avatar_match:
                        author_avatar_url = unescape(author_avatar_match.group(1))
                        logging.info(f"å°è¯•ä¸‹è½½åŸä½œè€…å¤´åƒ - URL: {author_avatar_url}")
                        avatar_path = download_avatar(author_avatar_url, author)
                        if avatar_path:
                            logging.info(f"æˆåŠŸä¸‹è½½åŸä½œè€…å¤´åƒ: {avatar_path}")
                        else:
                            logging.warning(f"ä¸‹è½½åŸä½œè€…å¤´åƒå¤±è´¥: {author}")
                    else:
                        logging.warning(f"æœªæ‰¾åˆ°åŸä½œè€…å¤´åƒURL: {author}")

            else:
                # 1. é¦–å…ˆåˆ†ç¦»ä¸»æ¨æ–‡å†…å®¹å’Œå¼•ç”¨å†…å®¹
                desc_main_only = re.split(r'<div class="rsshub-quote">', desc)[0]
                
                # 2. ä½¿ç”¨æ›´æ¿€è¿›çš„æ¸…ç†æ–¹å¼ - æŸ¥æ‰¾ä½œè€…åå’Œå†’å·åçš„ä½ç½®ï¼Œåªä¿ç•™ä¹‹åçš„å†…å®¹
                author_pattern = fr'{re.escape(author)}[:ï¼š]\s*'
                match = re.search(author_pattern, desc_main_only)
                if match:
                    desc_main_only = desc_main_only[match.end():]
                
                # 3. ç§»é™¤æ‰€æœ‰å¯èƒ½çš„å¼€å¤´HTMLæ ‡ç­¾å’Œç©ºç™½
                #desc_main_only = re.sub(r'^.*?<br>', '', desc_main_only, flags=re.DOTALL)  # ç§»é™¤å¼€å¤´åˆ°ç¬¬ä¸€ä¸ª<br>ä¹‹é—´çš„æ‰€æœ‰å†…å®¹
                #desc_main_only = re.sub(r'^<[^>]+>', '', desc_main_only)  # ç§»é™¤å‰©ä½™çš„å¼€å¤´HTMLæ ‡ç­¾
                desc_main_only = re.sub(fr'^.*?{re.escape(author_clean)}[:ï¼š]\s*', '', desc_main_only)
                desc_main_only = desc_main_only.lstrip()  # ç§»é™¤å¼€å¤´ç©ºç™½

                # ä»desc_main_onlyä¸­æå–å¤´åƒURL
                m = re.search(r'<img[^>]+?src="([^"]+?/profile_images/[^"]+?)"[^>]*?>', desc)
                if m:
                    avatar_url = unescape(m.group(1))
                    # å…ˆå°è¯•è·å–æœ¬åœ°å¤´åƒ
                    avatar_path = get_avatar_by_author(author)
                    # å¦‚æœæœ¬åœ°æ²¡æœ‰,åˆ™ä¸‹è½½
                    if not avatar_path:
                        avatar_path = download_avatar(avatar_url, author)
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°URL,ä»ç„¶å°è¯•ä»æœ¬åœ°è·å–
                    avatar_path = get_avatar_by_author(author)

                # ä¼˜åŒ–å¼•ç”¨å—çš„æå–
                quote_block_match = re.search(r'<div class="rsshub-quote">(.*?)</div>', desc, re.DOTALL)
                if quote_block_match:
                    quote_block = quote_block_match.group(1)
                    # æå–å¼•ç”¨å†…å®¹ä¸­çš„å¤´åƒå’Œç”¨æˆ·å
                    quote_author_match = re.search(r'<img[^>]+?src="([^"]+?)"[^>]*?>([^:<]+?):', quote_block)
                    if quote_author_match:
                        quote_avatar_url = unescape(quote_author_match.group(1))
                        quoted_username = quote_author_match.group(2).strip()
                        # æ¸…ç†ç”¨æˆ·åï¼Œç§»é™¤æœ«å°¾çš„æ ‡è®°å’Œå†’å·
                        quoted_username = re.sub(r'<[^>]+>', '', quoted_username)
                        #quoted_username = re.sub(r'\s*(?:OFFICIAL|ã®ã“ã¨ã€‚?|[:ï¼š])\s*$', '', quoted_username)
                        quoted_username = re.sub(r'[\\/:*?"<>|\s]', '_', quoted_username.strip()).strip('_')
                        
                        logging.info(f"æå–åˆ°å¼•ç”¨ç”¨æˆ·ä¿¡æ¯ - ç”¨æˆ·å: {quoted_username}")
                        logging.info(f"æå–åˆ°å¼•ç”¨ç”¨æˆ·ä¿¡æ¯ - å¤´åƒURL: {quote_avatar_url}")
                        # ç›´æ¥æ¸…ç†å¼•ç”¨å†…å®¹ï¼ŒåŒ…æ‹¬ç”¨æˆ·åå‰ç¼€
                        quote_block = re.sub(fr'{re.escape(quoted_username)}[:ï¼š]\s*', '', quote_block)  # ç§»é™¤ç”¨æˆ·åå’Œå†’å·
                        quote_block = re.sub(r'^<img[^>]+>', '', quote_block)  # åªç§»é™¤å¤´åƒæ ‡ç­¾
                        quote_clean = clean_html(quote_block)
                        quote_zh = translate_text(quote_clean) if quote_clean else ''
                             
                        # ä¸‹è½½å¼•ç”¨ç”¨æˆ·å¤´åƒ
                        if quote_avatar_url.startswith("https://pbs.twimg.com/profile_images/"):
                            if not avatar_quote:
                                avatar_quote = download_avatar(quote_avatar_url, quoted_username)
                            avatar_quote = get_avatar_by_author(quoted_username)

                    
                    # æ¸…ç†å¼•ç”¨å†…å®¹ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„æ­£åˆ™è¡¨è¾¾å¼å»é™¤ä½œè€…ä¿¡æ¯
                    quote_block = re.sub(r'^<img[^>]+>', '', quote_block)
                    quote_clean = clean_html(quote_block)
                    quote_zh = translate_text(quote_clean) if quote_clean else ''
                else:
                    quote_block = ''
                    quote_clean = ''
                    quote_zh = ''
                    quoted_username = ''
                    avatar_quote = None


            # æå–ä¸»æ¨æ–‡å­—
            logging.info(f"[{author}] å¼€å§‹ç¿»è¯‘...")
            # --- åç»­å¤„ç†ä¿æŒåŸæœ‰é€»è¾‘ï¼Œä½†éœ€ç¡®ä¿desc_cleanä¸ºç©ºæ—¶å¤„ç† ---
            desc_clean = clean_html(desc_main_only) if desc_main_only else ''
            desc_zh = translate_text(desc_clean) if desc_clean else ''

            # --- Step 2: æå–è½¬æ¨å— html ---
            #quote_block_match = re.search(r'<div class="rsshub-quote">(.*?)</div>', desc, re.DOTALL)
            #quote_block = quote_block_match.group(1) if quote_block_match else ''


            logging.info(f"[{author}] ç¿»è¯‘å®Œæˆ.")


            # æå–è½¬æ¨ç”¨æˆ·å¤´åƒï¼ˆä» quote_block ä¸­ï¼‰
            logging.info("å¼€å§‹å°è¯•æå–è½¬æ¨å¤´åƒé“¾æ¥")
            #avatar_quote = None
            #quoted_username = ''  # æ·»åŠ æ­¤è¡Œï¼Œè®¾ç½®é»˜è®¤å€¼
            matches = re.findall(r'<img[^>]+src="([^"]+pbs\.twimg\.com/profile_images/[^"]+)"[^>]*>\s*([^:<\n]+)', quote_block)
            

            if matches:
                avatar_quote_url = matches[0][0]  # åªè·å–å¤´åƒURLï¼Œä¸è¦†ç›–ç”¨æˆ·å
                #quoted_username = re.sub(r'<[^>]+>', '', quoted_username)
                #quoted_username = re.sub(r'[\\/:*?"<>|\s]', '_', quoted_username.strip()).strip('_')

                logging.info(f"æå–åˆ°è½¬æ¨å¤´åƒé“¾æ¥ï¼š{avatar_quote_url}")
                logging.info(f"è½¬æ¨ç”¨æˆ·åç”¨äºå‘½åå¤´åƒæ–‡ä»¶ï¼š{quoted_username}")

                if quoted_username:  # ä½¿ç”¨ä¹‹å‰æå–çš„ç”¨æˆ·å
                    avatar_quote = download_avatar(avatar_quote_url, quoted_username)

                if avatar_quote and avatar_quote != SKIPPED_PROFILE_IMAGE_FLAG and os.path.exists(avatar_quote):
                    logging.info(f"è½¬æ¨å¤´åƒå·²å­˜åœ¨æˆ–è€…ä¸‹è½½æˆåŠŸï¼š{avatar_quote}")
                else:
                    logging.warning(f"è½¬æ¨å¤´åƒä¸‹è½½å¤±è´¥æˆ–æ— æ•ˆè·¯å¾„ï¼š{avatar_quote}")
            else:
                logging.warning("æœªæ‰¾åˆ°è½¬æ¨å¤´åƒ <img> æ ‡ç­¾æˆ–ç”¨æˆ·å")

            # æå–åª’ä½“é“¾æ¥ï¼Œè·³è¿‡éšè—å…ƒç´ 
            media_urls = []
            seen_urls = set()
            for tag in re.findall(r'(<img[^>]+>)', desc):
                if tag.startswith('<img width="0" height="0" hidden="true"'):
                    continue
                u = re.search(r'src="([^"]+)"', tag).group(1)
                u = unescape(u)
                if u.startswith("https://pbs.twimg.com/profile_images/"):
                    continue  # è·³è¿‡å¤´åƒ
                if u not in seen_urls:
                    media_urls.append(u)
                    seen_urls.add(u)

            
            # åŒç†å¤„ç† <video>
            for tag in re.findall(r'(<video[^>]+>.*?</video>)', desc, re.DOTALL):
                if 'hidden="true"' in tag or 'style="display:none"' in tag:
                    continue
                src_match = re.search(r'src="([^"]+)"', tag)
                if src_match:
                    u = src_match.group(1).replace('&amp;', '&')
                    if u not in seen_urls:
                        media_urls.append(u)
                        seen_urls.add(u)

            media_paths = [p for u in media_urls if (p := download_media(u, author))]

    
            # å¤„ç†æ—¶é—´è½¬æ¢
            try:
                pub_dt_gmt = datetime.strptime(pub_dt, '%a, %d %b %Y %H:%M:%S %Z')
                pub_dt_utc = pub_dt_gmt.replace(tzinfo=timezone.utc)
            except ValueError:
                pub_dt_gmt = datetime.strptime(pub_dt.rstrip(' GMT'), '%a, %d %b %Y %H:%M:%S')
                pub_dt_utc = pub_dt_gmt.replace(tzinfo=timezone.utc)

            beijing_tz = timezone(timedelta(hours=8))
            pub_dt_beijing = pub_dt_utc.astimezone(beijing_tz)
            
            WEEKDAY_MAP = {
                0: "æ˜ŸæœŸä¸€", 1: "æ˜ŸæœŸäºŒ", 2: "æ˜ŸæœŸä¸‰", 3: "æ˜ŸæœŸå››",
                4: "æ˜ŸæœŸäº”", 5: "æ˜ŸæœŸå…­", 6: "æ˜ŸæœŸå¤©"
            }
            weekday_cn = WEEKDAY_MAP[pub_dt_beijing.weekday()]
            beijing_time_str = f"{weekday_cn}ï¼Œ{pub_dt_beijing.year}.{pub_dt_beijing.month:02}.{pub_dt_beijing.day:02} {pub_dt_beijing.strftime('%H:%M:%S')}"
            

            # # ç”Ÿæˆæ¶ˆæ¯æ–‡æœ¬
            # message_text = '\n'.join([
            #     f"ğŸ‘¤ æ¥è‡ªç”¨æˆ·ï¼š{author}",                
            #     f"ğŸŒˆ åŸæ–‡å†…å®¹ï¼š\n{desc_clean}",
            #     f"ğŸŒˆ ç¿»è¯‘å†…å®¹ï¼š\n{desc_zh}",
            #     '',
            #     f"ğŸ•’ å‘å¸ƒæ—¶é—´ï¼š{beijing_time_str}",
            #     ' '.join(categories)
            # ])

            logging.info(f"[{author}] å¼€å§‹ç”Ÿæˆæ¶ˆæ¯å›¾ç‰‡...")
            # ç”Ÿæˆå›¾ç‰‡å¹¶å‘é€
            img_path = text_to_image_html(
                author=author,
                author_id=author_id,  # æ–°å¢
                desc_clean=desc_clean,
                desc_zh=desc_zh,
                quote_clean=quote_clean,
                quote_zh=quote_zh,
                categories=categories,
                beijing_time_str=beijing_time_str,
                avatar_path=avatar_path,
                avatar_quote=avatar_quote,
                quoted_username=quoted_username,
                is_retweet=bool(rt_pattern.search(desc))  # æ–°å¢å‚æ•°
            )
            logging.info(f"[{author}] æ¶ˆæ¯å›¾ç‰‡ç”Ÿæˆå®Œæˆ: {img_path}")

            for target_id in TARGET_IDs_list:
                logging.info(f"[{author}] å¼€å§‹å‘ç¾¤ {target_id} å‘é€...")                
                # ä¼˜å…ˆå‘é€å›¾ç‰‡æ¶ˆæ¯
                if img_path:
                    img_message = upload_image(img_path, session_key)
                    if img_message:
                        # ç«‹å³å‘é€å›¾ç‰‡å’Œé“¾æ¥
                        send_message(session_key, target_id, [{"type": "Image", "imageId": img_message}])
                        send_message(session_key, target_id, [{"type": "Plain", "text": f"ğŸ”— åŸæ–‡é“¾æ¥ï¼š{link}"}])
                        print(f"ç«‹å³å‘é€ç¿»è¯‘åå›¾ç‰‡åˆ°ç¾¤ {target_id} | ç”¨æˆ·ï¼š{author}")
                    else:
                        logging.error(f"[{author}] ä¸Šä¼ ç¿»è¯‘å›¾ç‰‡åˆ°ç¾¤ {target_id} å¤±è´¥")

                # å¤„ç†åª’ä½“æ–‡ä»¶
                for media_path in media_paths:
                    filename = os.path.basename(media_path)
                    if filename in uploaded:
                        logging.info(f"[{author}] åª’ä½“æ–‡ä»¶ {filename} å·²ä¸Šä¼ è¿‡ï¼Œè·³è¿‡")
                        continue
                    if media_path == SKIPPED_PROFILE_IMAGE_FLAG:
                        continue

                    if media_path.lower().endswith(('.jpg', '.png', '.jpeg', '.gif')):
                        img_id = upload_image(media_path, session_key)
                        if img_id:
                            send_message(session_key, target_id, [{"type": "Image", "imageId": img_id}])
                    elif media_path.lower().endswith(('.mp4', '.mkv', '.avi', '.mov')):
                        vid_id = upload_video(media_path, session_key, target_id)
                        if vid_id:
                            send_message(session_key, target_id, [{"type": "File", "id": vid_id}])
                    else:
                        logging.warning(f"æœªè¯†åˆ«çš„åª’ä½“ç±»å‹ï¼š{media_path}")
            seen.add(link)
            logging.info(f"[{author}] é¡¹ç›® {link} å¤„ç†å®Œæ¯•ï¼Œæ ‡è®°ä¸ºå·²çœ‹ã€‚")
            save_seen(seen)
            save_uploaded(uploaded)

# æ¶ˆæ¯å›¾ç‰‡åŒ–
def text_to_image_html(
    author: str,
    author_id: str,  # æ–°å¢å‚æ•°
    desc_clean: str,
    desc_zh: str,
    quote_clean: str = '',
    quote_zh: str = '',
    categories: list = None,
    output_path: str = "./output",
    beijing_time_str=None,
    avatar_path: str = None,
    avatar_quote: str = None,
    quoted_username: str = '',
    is_retweet: bool = False  # æ–°å¢å‚æ•°
):
    # ç”Ÿæˆä»¥æ—¶é—´æˆ³å‘½åçš„æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{timestamp}.png"
    filepath = os.path.join(output_path, filename)
    output_path = os.path.abspath(output_path)
    filepath = os.path.join(output_path, filename)
    os.makedirs(output_path, exist_ok=True)

    # ç§»é™¤ç›¸å¯¹è·¯å¾„è½¬æ¢å‡½æ•°ï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ç»å¯¹è·¯å¾„
    if avatar_path:
        avatar_path = os.path.abspath(avatar_path)
    if avatar_quote:
        avatar_quote = os.path.abspath(avatar_quote)


    # å†…åµŒ hashtag é«˜äº®é€»è¾‘ï¼ˆ#tag â†’ è“è‰²ï¼‰
    def highlight_hashtags(text: str) -> str:
        # ä¿®æ”¹æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ’é™¤ < å­—ç¬¦ï¼Œè¿™æ ·å°±ä¸ä¼šåŒ¹é…åˆ° <br>
        return re.sub(r'#([^<\s]+)', r'<span style="color:#1da1f2;">#\1</span>', text)




    # å¯¹æ‰€æœ‰æ–‡æœ¬å†…å®¹è¿›è¡Œå¤„ç†
    desc_clean = merge_consecutive_br(desc_clean)
    desc_zh = merge_consecutive_br(desc_zh)
    quote_clean = merge_consecutive_br(quote_clean)
    quote_zh = merge_consecutive_br(quote_zh)
    
    # æ¸²æŸ“ hashtag
    desc_clean = highlight_hashtags(desc_clean)
    desc_zh = highlight_hashtags(desc_zh)
    quote_clean = highlight_hashtags(quote_clean)
    quote_zh = highlight_hashtags(quote_zh)


    # åŠ è½½å¤–éƒ¨ HTML æ¨¡æ¿

    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = Path(__file__).resolve().parent


    # è®¾ç½®æ¨¡æ¿æ–‡ä»¶å¤¹çš„ç»å¯¹è·¯å¾„
    template_dir = os.path.join(script_dir, "html")

    # æ–°å¢ï¼šè¯»å–CSSå’ŒJSæ–‡ä»¶å†…å®¹
    def load_resource(filename):
        resource_path = os.path.join(template_dir, filename)
        try:
            with open(resource_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logging.error(f"åŠ è½½èµ„æºå¤±è´¥: {resource_path} - {str(e)}")
            return ""
        
    # åŠ è½½CSSå’ŒJSå†…å®¹
    css_content = load_resource("css2.css")
    js_content = load_resource("browser@4.js")

    # åŠ è½½æ¨¡æ¿
    env = Environment(loader=FileSystemLoader(template_dir))
    template_name = 'seiyuu.html' if quote_clean.strip() or quote_zh.strip() else 'no-quote.html'
    template = env.get_template(template_name)
    #avatar_base64 = image_to_base64(avatar_path) if avatar_path else None

    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    logging.info(f"ç”Ÿæˆå›¾ç‰‡å‚æ•° | author_id={author_id} | avatar_path={avatar_path}")



    # å‡†å¤‡ç¿»è¯‘æ¥æºæç¤ºæ ·å¼
    if not desc_clean.strip() and not desc_zh.strip() and not quote_clean.strip() and not quote_zh.strip():
        # å¦‚æœæ²¡æœ‰ä»»ä½•æ–‡å­—å†…å®¹,åˆ™ä¼ å…¥ç©ºç™½çš„ translate_source
        translate_source = ''
    elif is_retweet:
        translate_source = "å·²è½¬æ¨"
    else:
        translate_source = '''
    <div class="text-blue-500 text-sm flex items-center">
        ç”±  
        <div class="flex flex-row items-center gap-2">
            <div class="h-9 w-9 text-gray-500" style="flex: none">
                <svg viewBox="0 0 30 30" width="30" height="30" xmlns="http://www.w3.org/2000/svg" class="fill-current">
                    <path id="path" d="M27.501 8.46875C27.249 8.3457 27.1406 8.58008 26.9932 8.69922C26.9434 8.73828 26.9004 8.78906 26.8584 8.83398C26.4902 9.22852 26.0605 9.48633 25.5 9.45508C24.6787 9.41016 23.9785 9.66797 23.3594 10.2969C23.2275 9.52148 22.79 9.05859 22.125 8.76172C21.7764 8.60742 21.4238 8.45312 21.1807 8.11719C21.0098 7.87891 20.9639 7.61328 20.8779 7.35156C20.8242 7.19336 20.7695 7.03125 20.5879 7.00391C20.3906 6.97266 20.3135 7.13867 20.2363 7.27734C19.9258 7.84375 19.8066 8.46875 19.8174 9.10156C19.8447 10.5234 20.4453 11.6562 21.6367 12.4629C21.7725 12.5547 21.8076 12.6484 21.7646 12.7832C21.6836 13.0605 21.5869 13.3301 21.501 13.6074C21.4473 13.7852 21.3662 13.8242 21.1768 13.7461C20.5225 13.4727 19.957 13.0684 19.458 12.5781C18.6104 11.7578 17.8438 10.8516 16.8877 10.1426C16.6631 9.97656 16.4395 9.82227 16.207 9.67578C15.2314 8.72656 16.335 7.94727 16.5898 7.85547C16.8574 7.75977 16.6826 7.42773 15.8193 7.43164C14.957 7.43555 14.167 7.72461 13.1611 8.10938C13.0137 8.16797 12.8594 8.21094 12.7002 8.24414C11.7871 8.07227 10.8389 8.0332 9.84766 8.14453C7.98242 8.35352 6.49219 9.23633 5.39648 10.7441C4.08105 12.5547 3.77148 14.6133 4.15039 16.7617C4.54883 19.0234 5.70215 20.8984 7.47559 22.3633C9.31348 23.8809 11.4307 24.625 13.8457 24.4824C15.3125 24.3984 16.9463 24.2012 18.7881 22.6406C19.2529 22.8711 19.7402 22.9629 20.5498 23.0332C21.1729 23.0918 21.7725 23.002 22.2373 22.9062C22.9648 22.752 22.9141 22.0781 22.6514 21.9531C20.5186 20.959 20.9863 21.3633 20.5605 21.0371C21.6445 19.752 23.2783 18.418 23.917 14.0977C23.9668 13.7539 23.9238 13.5391 23.917 13.2598C23.9131 13.0918 23.9512 13.0254 24.1445 13.0059C24.6787 12.9453 25.1973 12.7988 25.6738 12.5352C27.0557 11.7793 27.6123 10.5391 27.7441 9.05078C27.7637 8.82422 27.7402 8.58789 27.501 8.46875ZM15.46 21.8613C13.3926 20.2344 12.3906 19.6992 11.9766 19.7227C11.5898 19.7441 11.6592 20.1875 11.7441 20.4766C11.833 20.7617 11.9492 20.959 12.1123 21.209C12.2246 21.375 12.3018 21.623 12 21.8066C11.334 22.2207 10.1768 21.668 10.1221 21.6406C8.77539 20.8477 7.64941 19.7988 6.85547 18.3652C6.08984 16.9844 5.64453 15.5039 5.57129 13.9238C5.55176 13.541 5.66406 13.4062 6.04297 13.3379C6.54199 13.2461 7.05762 13.2266 7.55664 13.2988C9.66602 13.6074 11.4619 14.5527 12.9668 16.0469C13.8262 16.9004 14.4766 17.918 15.1465 18.9121C15.8584 19.9688 16.625 20.9746 17.6006 21.7988C17.9443 22.0879 18.2197 22.3086 18.4824 22.4707C17.6895 22.5586 16.3652 22.5781 15.46 21.8613ZM16.4502 15.4805C16.4502 15.3105 16.5859 15.1758 16.7568 15.1758C16.7949 15.1758 16.8301 15.1836 16.8613 15.1953C16.9033 15.2109 16.9424 15.2344 16.9727 15.2695C17.0273 15.3223 17.0586 15.4004 17.0586 15.4805C17.0586 15.6504 16.9229 15.7852 16.7529 15.7852C16.582 15.7852 16.4502 15.6504 16.4502 15.4805ZM19.5273 17.0625C19.3301 17.1426 19.1328 17.2129 18.9434 17.2207C18.6494 17.2344 18.3281 17.1152 18.1533 16.9688C17.8828 16.7422 17.6895 16.6152 17.6074 16.2168C17.5732 16.0469 17.5928 15.7852 17.623 15.6348C17.6934 15.3105 17.6152 15.1035 17.3877 14.9141C17.2012 14.7598 16.9658 14.7188 16.7061 14.7188C16.6094 14.7188 16.5205 14.6758 16.4541 14.6406C16.3457 14.5859 16.2568 14.4512 16.3418 14.2852C16.3691 14.2324 16.501 14.1016 16.5322 14.0781C16.8838 13.877 17.29 13.9434 17.666 14.0938C18.0146 14.2363 18.2773 14.498 18.6562 14.8672C19.0439 15.3145 19.1133 15.4395 19.334 15.7734C19.5078 16.0371 19.667 16.3066 19.7754 16.6152C19.8408 16.8066 19.7559 16.9648 19.5273 17.0625Z" fill-rule="nonzero" fill="#4D6BFE"></path>
                </svg>
            </div>
        </div>
        ç¿»è¯‘è‡ªæ—¥è¯­
    </div>
    '''

    # æ¸²æŸ“ HTML
    rendered_html = template.render(
        author=author,
        author_id=author_id,  # æ–°å¢
        desc_clean=desc_clean,
        desc_zh=desc_zh,
        quote_clean=quote_clean,
        quote_zh=quote_zh,
        beijing_time_str=beijing_time_str,
        categories=categories,
        avatar_path=avatar_path,
        avatar_quote=avatar_quote,
        quoted_username=quoted_username,
        translate_source=translate_source,
        css_content=css_content,
        js_content=js_content  # æ–°å¢å‚æ•°
    )

    # Step 1: ç”Ÿæˆå¤§å°ºå¯¸æˆªå›¾
    hti = Html2Image(output_path=output_path, size=(2160, 8000))
    try:
        hti.screenshot(html_str=rendered_html, save_as=filename)
    except Exception as e:
        logging.error(f"HTML è½¬å›¾ç‰‡å¤±è´¥ï¼š{e}")
        return None

    # Step 2: è£å‰ªç™½è‰²ç©ºç™½åŒºåŸŸ
    img = Image.open(filepath)
    gray = img.convert('L')
    bbox = gray.point(lambda x: 0 if x == 255 else 255).getbbox()

    if bbox:
        img = img.crop(bbox)
        img.save(filepath)

    return filepath




# -------------------
# æ–°å¢ï¼šä¸‹è½½å¤´åƒå‡½æ•°
# -------------------
def modify_avatar_url(url: str) -> str:
    """å°†å¤´åƒURLä¸­çš„ _normal æ›¿æ¢ä¸º _400x400 ä»¥è·å–é«˜æ¸…ç‰ˆæœ¬"""
    if not url:
        return url
    return url.replace('_normal.', '_400x400.')

def download_avatar(url: str, author: str) -> str:
    if not url:
        logging.warning(f"å¤´åƒURLä¸ºç©ºï¼Œæ— æ³•ä¸‹è½½ï¼š{author}")
        return None

    # ä¿®æ”¹URLè·å–é«˜æ¸…ç‰ˆæœ¬
    url = modify_avatar_url(url)

    os.makedirs(AVATAR_DIR, exist_ok=True)  # ç¡®ä¿ avatar ç›®å½•å­˜åœ¨

    url = unescape(url.replace('&amp;', '&'))
    parsed_url = urlparse(url)
    ext = os.path.splitext(parsed_url.path)[1] or ".png"  # ä¿ç•™æ‰©å±•åæˆ–é»˜è®¤ä¸º.png
    filename = f"{author}{ext}"
    path = os.path.join(AVATAR_DIR, filename)

    # å¦‚æœå¤´åƒæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™ç›´æ¥è¿”å›è·¯å¾„ï¼Œä¸é‡æ–°ä¸‹è½½
    if os.path.exists(path):
        logging.info(f"å¤´åƒå·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ï¼š{filename}")
        return path

    try:
        resp = requests.get(
            url,
            proxies=proxies,
            stream=True,
            timeout=15,
            verify=False,
            headers={'User-Agent': 'Mozilla/5.0'}
        )
        resp.raise_for_status()
        with open(path, 'wb') as f:
            for chunk in resp.iter_content(1024):
                f.write(chunk)
        logging.info(f"å¤´åƒä¸‹è½½æˆåŠŸï¼š{filename}")
        return path
    except Exception as e:
        logging.warning(f"å¤´åƒä¸‹è½½å¤±è´¥ï¼š{str(e)}")
        return None

def image_to_base64(image_path: str) -> Optional[str]:
    if not image_path or not os.path.exists(image_path):
        return None
    try:
        with open(image_path, "rb") as img_file:
            encoded = base64.b64encode(img_file.read()).decode("utf-8")
            ext = os.path.splitext(image_path)[1][1:].lower()  # jpg/png/svg ç­‰
            return f"data:image/{ext};base64,{encoded}"
    except Exception as e:
        logging.warning(f"Base64 ç¼–ç å¤±è´¥ï¼š{e}")
        return None


def start_immediate_tasks():
# å¯åŠ¨æ—¶ç«‹å³æ‰«ææ›´æ–°
    Twitter_seiyuu()

# è°ƒåº¦å…¥å£
if __name__ == '__main__':
    schedule.every(1).minutes.do(Twitter_seiyuu)
    # å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä»»åŠ¡
    start_immediate_tasks()
    while True:
        schedule.run_pending()
        time.sleep(1)
